{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# collecting DMO gilt announcements from 2005 to present\n",
    "\n",
    "url = 'https://www.dmo.gov.uk/publications/?offset=0&itemsPerPage=1000000&parentFilter=1433&childFilter=1433|1450&startMonth=1&startYear=2005&endMonth=7&endYear=2021'\n",
    "html = requests.get(url).content\n",
    "df_list = pd.read_html(html)\n",
    "dates = df_list[-1]\n",
    "dates.to_excel('gilt_announcements_20052021.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting dates dataset, some steps of data cleaning were done manually therefore I am importing the excel file\n",
    "\n",
    "dates = dates.dropna()\n",
    "dates = dates[dates['Publication title'].str.contains(\"Treasury Gilt\")]\n",
    "dates = dates[~dates['Publication title'].str.contains(\"Result\")]\n",
    "dates['amount'] = dates['Publication title'].str.split('Auction of ').str[1]\n",
    "dates['amount2'] = dates['Publication title'].str.split('Auctions of ').str[1]\n",
    "dates.loc[dates['amount'].isnull(), 'amount'] = dates['amount2']\n",
    "dates['amount'] = dates['amount'].str.split('of').str[0]\n",
    "dates.loc[dates['Publication title'].str.contains(\"Index\"), 'index'] = 1\n",
    "dates.loc[~dates['Publication title'].str.contains(\"Index\"), 'index'] = 0\n",
    "dates.amount.replace('\\D+','',regex=True,inplace=True)\n",
    "del dates['amount2']\n",
    "dates = dates.dropna()\n",
    "dates['Date published'] = pd.to_datetime(dates['Date published'],infer_datetime_format=True)\n",
    "\n",
    "dates = pd.read_excel('datescheck4.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formating covariates\n",
    "\n",
    "exchange_rates = pd.read_csv('exchangerates.xlsm - ObservationData (1).csv')\n",
    "exchange_rates = exchange_rates.T\n",
    "new_header = exchange_rates.iloc[0] #grab the first row for the header\n",
    "exchange_rates = exchange_rates[1:] #take the data less the header row\n",
    "exchange_rates.columns = new_header\n",
    "exchange_rates = exchange_rates.reset_index()\n",
    "exchange_rates['index'] = pd.to_datetime(exchange_rates['index'] )\n",
    "exchange_rates['Date'] = exchange_rates['index'].copy()\n",
    "del exchange_rates['index']\n",
    "\n",
    "ftse = pd.read_csv('ftse100close.csv').T\n",
    "ftse = ftse[1:]\n",
    "ftse = ftse.reset_index()\n",
    "ftse.columns = ['Date','close']\n",
    "ftse['close'] = ftse['close'].str.replace(',','.')\n",
    "ftse = ftse.dropna()\n",
    "ftse['close'] = pd.to_numeric(ftse['close'])\n",
    "ftse['Date'] = pd.to_datetime(ftse['Date'],infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the breakeven rates datasets\n",
    "\n",
    "spot_infla_2005 = pd.read_excel('Inflation_Daily_2005-2015.xlsx', '4')\n",
    "spot_infla_2005 = spot_infla_2005.dropna()\n",
    "spot_infla_pres = pd.read_excel('2016-present_infla.xlsx', '4')\n",
    "spot_infla_pres = spot_infla_pres.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting \n",
    "\n",
    "implied_infla = implied_infla.set_index('Date')\n",
    "# 5 year bonds\n",
    "implied_infla[5].plot()\n",
    "\n",
    "# 15 year bonds\n",
    "implied_infla[15].plot()\n",
    "\n",
    "# 25 year bonds \n",
    "implied_infla[25].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging with covariates\n",
    "\n",
    "implied_infla = implied_infla.reset_index()\n",
    "implied_infla = pd.merge(implied_infla, exchange_rates, on='Date')\n",
    "implied_infla['Exchange Index'] = implied_infla['Exchange Index'].str.replace(r',', r'.').astype('float') \n",
    "implied_infla['Exchange Index'] = pd.to_numeric(implied_infla['Exchange Index'])\n",
    "\n",
    "\n",
    "implied_infla = implied_infla.reset_index()\n",
    "implied_infla = pd.merge(implied_infla, ftse, on='Date')\n",
    "implied_infla['close'] = pd.to_numeric(implied_infla['close'])\n",
    "implied_infla['close_pctchange'] = implied_infla['close'].pct_change()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more plotting\n",
    "import numpy as np\n",
    "\n",
    "implied_infla = implied_infla.set_index('Date')\n",
    "\n",
    "\n",
    "np.log10(implied_infla[5]).plot()\n",
    "np.log10(implied_infla['Exchange Index']).plot()\n",
    "np.log10(implied_infla['close']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only using dates between the ones available with inflation expectations\n",
    "\n",
    "dates = dates[dates['Date published'] <= '2021-03-31']\n",
    "dates = dates[dates['Date published'] >= '2011-01-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the correlation matrix\n",
    "\n",
    "a = implied_infla.corr()\n",
    "a.to_excel('correlation_matrix.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the pre-period and post-period dates\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "dates['pre_period'] = dates['Date published'] - pd.DateOffset(days=20)\n",
    "dates['post_period'] = dates['Date published'] + pd.DateOffset(days=3)\n",
    "dates['datepublished1'] = dates['Date published'] + pd.DateOffset(days=1)\n",
    "\n",
    "from pandas.tseries.offsets import BDay\n",
    "dates.pre_period = dates.pre_period.map(lambda x : x + 0*BDay())\n",
    "dates.post_period = dates.post_period.map(lambda x : x + 0*BDay())\n",
    "dates.datepublished1 = dates.datepublished1.map(lambda x : x + 0*BDay())\n",
    "dates['Date published'] = dates['Date published'].map(lambda x : x + 0*BDay())\n",
    "\n",
    "# pre-period and post-period dates should also be between bounds of inflation exp. range\n",
    "dates = dates[dates['post_period'] <= '2021-03-31']\n",
    "dates = dates[dates['pre_period'] >= '2011-01-04']\n",
    "dates = dates.sort_values(by=['Date published'])\n",
    "\n",
    "# transforming to lists for the main algorithm\n",
    "implied_infla = implied_infla.reset_index()\n",
    "a = dates['Date published'].tolist()\n",
    "b = dates['pre_period'].tolist()\n",
    "c = dates['post_period'].tolist()\n",
    "d = dates['datepublished1'].tolist()\n",
    "\n",
    "implied_infla['Date'] = pd.to_datetime(implied_infla['Date'],infer_datetime_format=True)\n",
    "implied_infla = implied_infla[implied_infla['Date'] <= '2021-03-31']\n",
    "implied_infla = implied_infla[implied_infla['Date'] >= '2011-01-04']\n",
    "implied_infla = implied_infla.set_index(\"Date\")\n",
    "\n",
    "implied_infla.columns = [str(col) + '_year' for col in implied_infla.columns]\n",
    "implied_infla.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the lists with all the years to be analyzed and their covariates \n",
    "year3 = implied_infla[['3_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year4 = implied_infla[['4_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year5 = implied_infla[['5_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year6 = implied_infla[['6_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year7 = implied_infla[['7_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year8 = implied_infla[['8_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year9 = implied_infla[['9_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year10 = implied_infla[['10_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year11 = implied_infla[['11_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year12 = implied_infla[['12_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year13 = implied_infla[['13_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year14 = implied_infla[['14_year','25_year','Exchange Index_year', 'close_year']]\n",
    "year15 = implied_infla[['15_year','25_year','Exchange Index_year', 'close_year']]\n",
    "\n",
    "list_years = [year3,year4,year5,year6,year7,year8,year9,year10,\n",
    "             year11,year12,year13,year14,year15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main BSTS algorithm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from causalimpact import CausalImpact\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "res = dates[['Date published','pre_period', 'post_period','amount', 'index']]\n",
    "\n",
    "for x in range(0, len(list_years)):\n",
    "    print(list_years[x].columns[0])\n",
    "    \n",
    "    params = []\n",
    "    errors = []\n",
    "    list2 = []\n",
    "    data = list_years[x].copy()\n",
    "    data.columns = ['y', 'X1', 'X2', 'X3']\n",
    "    \n",
    "    for n in range(0,len(dates)):\n",
    "        pre_period = [b[n], a[n]]\n",
    "        post_period = [d[n], c[n]]\n",
    "        try:\n",
    "            ci = CausalImpact(data, pre_period, post_period)\n",
    "            r1 = re.search('tail-area probability p: (.+?)\\nPosterior prob.', ci.summary())\n",
    "            params.append(r1.group(1))\n",
    "            list2.append(ci.summary())\n",
    "\n",
    "        except ValueError as err:\n",
    "            params.append('NaN')\n",
    "            errors.append(err)\n",
    "            list2.append('error')\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        print(len(params))\n",
    "        print('sig numbers:', len([float(n) for n in params if float(n) <= 0.05]))\n",
    "        print('non-sig numbers:', len([float(n) for n in params if float(n) > 0.05]))\n",
    "        print(len(errors))\n",
    "    except TypeError:\n",
    "        print('weird')\n",
    "    \n",
    "    listtemp = []\n",
    "    for num in range(0, len(list2)):\n",
    "        try:\n",
    "            actual_average = re.search('\\nActual[\\t ]*(.+?)[\\t ]', list2[num]).group(1)\n",
    "            actual_cumulative = re.search('\\nActual[\\t ]*\\d.\\d*[\\t ]*(.+?)\\nPrediction', list2[num]).group(1)\n",
    "\n",
    "            rel_average = re.search('\\n\\nRelative effect \\(s.d.\\)[\\t ]*(.+?)\\s', list2[num]).group(1)\n",
    "            rel_cumulative = re.search('\\n\\nRelative effect \\(s.d.\\).*-.*\\)[\\t ]*(.+?)\\s', list2[num]).group(1)\n",
    "\n",
    "            abs_average = re.search('\\nAbsolute effect \\(s.d.\\)[\\t ]*(.+?)\\s', list2[num]).group(1)\n",
    "            abs_cumulative = re.search('\\nAbsolute effect \\(s.d.\\).*-.*\\)[\\t ]*(.+?)\\s', list2[num]).group(1)\n",
    "\n",
    "            listtemp.append([actual_average,actual_cumulative,rel_average,rel_cumulative,abs_average,abs_cumulative])\n",
    "        \n",
    "        except:\n",
    "            listtemp.append(['error','error','error','error','error','error'])\n",
    "            \n",
    "    if x == 0:\n",
    "        res['year3_p-value'] = params\n",
    "        \n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year3' for col in dtemp.columns]\n",
    "        \n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "    elif x == 1:\n",
    "        res['year4_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year4' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "    elif x == 2:\n",
    "        res['year5_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year5' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "    elif x == 3:\n",
    "        res['year6_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year6' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "    elif x == 4:\n",
    "        res['year7_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year7' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "    elif x == 5:\n",
    "        res['year8_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year8' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "    elif x == 6:\n",
    "        res['year9_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year9' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "    elif x == 7:\n",
    "        res['year10_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year10' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    elif x == 8:\n",
    "        res['year11_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year11' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "    elif x == 9:\n",
    "        res['year12_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year12' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "        \n",
    "    elif x == 10:\n",
    "        res['year13_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year13' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    elif x == 11:\n",
    "        res['year14_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year14' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)   \n",
    "        \n",
    "    elif x == 12:\n",
    "        res['year15_p-value'] = params\n",
    "        dtemp = pd.DataFrame.from_records(listtemp, columns =['actual_average','actual_cumulative','rel_average',\n",
    "                                                              'rel_cumulative','abs_average','abs_cumulative'])\n",
    "        dtemp.columns = [str(col) + 'year15' for col in dtemp.columns]\n",
    "        res = pd.concat([res.reset_index(drop=True),dtemp.reset_index(drop=True)], axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T TESTS AND MORE RESULTS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes for all maturities\n",
    "\n",
    "res3year = res[['Date published', 'pre_period', 'post_period','amount', 'index','year3_p-value', \n",
    "             'actual_averageyear3', 'actual_cumulativeyear3',\n",
    "             'rel_averageyear3', 'rel_cumulativeyear3', 'abs_averageyear3','abs_cumulativeyear3']]\n",
    "res4year = res[['Date published', 'pre_period', 'post_period','amount', 'index','year4_p-value', \n",
    "             'actual_averageyear4', 'actual_cumulativeyear4',\n",
    "             'rel_averageyear4', 'rel_cumulativeyear4', 'abs_averageyear4','abs_cumulativeyear4']]\n",
    "res5year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "             'year5_p-value', 'actual_averageyear5',\n",
    "       'actual_cumulativeyear5', 'rel_averageyear5', 'rel_cumulativeyear5',\n",
    "       'abs_averageyear5', 'abs_cumulativeyear5']]\n",
    "res6year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "                'year6_p-value', \n",
    "             'actual_averageyear6', 'actual_cumulativeyear6',\n",
    "             'rel_averageyear6', 'rel_cumulativeyear6', 'abs_averageyear6','abs_cumulativeyear6']]\n",
    "res7year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "             'year7_p-value','actual_averageyear7', 'actual_cumulativeyear7', 'rel_averageyear7',\n",
    "       'rel_cumulativeyear7', 'abs_averageyear7', 'abs_cumulativeyear7']]\n",
    "res8year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "             'year8_p-value','actual_averageyear8', 'actual_cumulativeyear8', 'rel_averageyear8',\n",
    "       'rel_cumulativeyear8', 'abs_averageyear8', 'abs_cumulativeyear8']]\n",
    "res9year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "             'year9_p-value','actual_averageyear9', 'actual_cumulativeyear9', 'rel_averageyear9',\n",
    "       'rel_cumulativeyear9', 'abs_averageyear9', 'abs_cumulativeyear9']]\n",
    "res10year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "              'year10_p-value', 'actual_averageyear10', 'actual_cumulativeyear10',\n",
    "       'rel_averageyear10', 'rel_cumulativeyear10', 'abs_averageyear10',\n",
    "       'abs_cumulativeyear10']]\n",
    "res11year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "              'year11_p-value', 'actual_averageyear11', 'actual_cumulativeyear11',\n",
    "       'rel_averageyear11', 'rel_cumulativeyear11', 'abs_averageyear11',\n",
    "       'abs_cumulativeyear11']]\n",
    "res12year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "              'year12_p-value', 'actual_averageyear12', 'actual_cumulativeyear12',\n",
    "       'rel_averageyear12', 'rel_cumulativeyear12', 'abs_averageyear12',\n",
    "       'abs_cumulativeyear12']]\n",
    "res13year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "              'year13_p-value', 'actual_averageyear13', 'actual_cumulativeyear13',\n",
    "       'rel_averageyear13', 'rel_cumulativeyear13', 'abs_averageyear13',\n",
    "       'abs_cumulativeyear13']]\n",
    "res14year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "              'year14_p-value', 'actual_averageyear14', 'actual_cumulativeyear14',\n",
    "       'rel_averageyear14', 'rel_cumulativeyear14', 'abs_averageyear14',\n",
    "       'abs_cumulativeyear14']]\n",
    "res15year = res[['Date published', 'pre_period', 'post_period','amount', 'index',\n",
    "              'year15_p-value', 'actual_averageyear15', 'actual_cumulativeyear15',\n",
    "       'rel_averageyear15', 'rel_cumulativeyear15', 'abs_averageyear15',\n",
    "       'abs_cumulativeyear15']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data collected on the DMO pdfs\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"list1finally.txt\", \"rb\") as fp:   # Unpickling\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "#listdf = pd.DataFrame.from_records(data)\n",
    "\n",
    "listdf = pd.read_excel('listdf3.xlsx')\n",
    "listdf['Date published'] = pd.to_datetime(listdf['Date published'])\n",
    "listdf.columns = ['Date published', 'avg_auction_value', 'supposed_auction_value_todate', 'total_sales_todate',\n",
    "                             'total_auctions_planned','total_auctions_remaining','total_planned_sales','total_sales_remaining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here all the cells will be the same except for the different maturities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3year = res[['Date published', 'pre_period', 'post_period','amount', 'index','year3_p-value', \n",
    "             'actual_averageyear3', 'actual_cumulativeyear3',\n",
    "             'rel_averageyear3', 'rel_cumulativeyear3', 'abs_averageyear3','abs_cumulativeyear3']]\n",
    "\n",
    "res3year = res3year[~res3year['actual_averageyear3'].str.contains(\"error\")]\n",
    "res3year = res3year.dropna(subset = ['year3_p-value'])\n",
    "res3year['rel_cumulativeyear3'] = res3year['rel_cumulativeyear3'].str.replace(r'%', r'').astype('float') / 100.0\n",
    "res3year['rel_averageyear3'] = res3year['rel_averageyear3'].str.replace(r'%', r'').astype('float') / 100.0\n",
    "res3year['actual_cumulativeyear3'] = pd.to_numeric(res3year['actual_cumulativeyear3'])\n",
    "res3year['actual_averageyear3'] = pd.to_numeric(res3year['actual_averageyear3'])\n",
    "res3year['abs_cumulativeyear3'] = pd.to_numeric(res3year['abs_cumulativeyear3'])\n",
    "res3year['abs_averageyear3'] = pd.to_numeric(res3year['abs_averageyear3'])\n",
    "res3year['year3_p-value'] = pd.to_numeric(res3year['year3_p-value'])\n",
    "res3year['sig'] = 1\n",
    "res3year.loc[res3year['year3_p-value'] > 0.05, 'sig'] = 0\n",
    "\n",
    "df1 = res3year.merge(listdf, on='Date published')\n",
    "df1['avg_auction_value'] = pd.to_numeric(df1['avg_auction_value'])\n",
    "df1['amount'] = pd.to_numeric(df1['amount'])\n",
    "df1['auctions_value_remaining'] = df1['total_sales_remaining']/(df1['total_auctions_remaining'])\n",
    "df1['total_sales_to_date'] = df1['total_planned_sales'] - df1['total_sales_remaining']\n",
    "df1['surprise_adaptive'] = df1['amount'] - df1['auctions_value_remaining']\n",
    "df1['surprise_naive'] = df1['amount'] - df1['avg_auction_value']\n",
    "df1['perc_surprise_adaptive'] = df1['amount']/df1['auctions_value_remaining']\n",
    "df1['perc_surprise_naive'] = df1['amount']/df1['avg_auction_value']\n",
    "\n",
    "df1_sig = df1[df1['sig'] == 1]\n",
    "nominals = df1[df1['index'] == 0]\n",
    "\n",
    "from scipy import stats\n",
    "x = nominals[nominals['sig'] == 0]['surprise_adaptive']\n",
    "y = nominals[nominals['sig'] == 1]['surprise_adaptive']\n",
    "print('adaptive', stats.ttest_ind(x, y,equal_var=True))\n",
    "\n",
    "x = nominals[nominals['sig'] == 0]['surprise_naive']\n",
    "y = nominals[nominals['sig'] == 1]['surprise_naive']\n",
    "print('naive', stats.ttest_ind(x, y,equal_var=True))\n",
    "print(''' \n",
    "''')\n",
    "print('mean of relative effect: ', \n",
    "      nominals[nominals['sig'] == 1]['rel_averageyear3'].mean())\n",
    "print(''' \n",
    "''')\n",
    "print('mean of surprise of non-significant gilt announcements: ', \n",
    "      nominals[nominals['sig'] == 0]['surprise_naive'].mean())\n",
    "print('mean of surprise of significant gilt announcements: ', \n",
    "      nominals[nominals['sig'] == 1]['surprise_naive'].mean())\n",
    "print(''' \n",
    "''')\n",
    "print('number of significant:', len(y))\n",
    "print('number of non-significant:', len(x))\n",
    "print(''' \n",
    "''')\n",
    "1/(nominals[nominals['sig'] == 1]['surprise_naive'].mean()/nominals[nominals['sig'] == 0]['surprise_naive'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
